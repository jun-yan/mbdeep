\documentclass[12pt, titlepage, reqno]{article}
\usepackage{authblk}
\usepackage[margin=1in]{geometry}
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\usepackage{array}
\RequirePackage{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs,longtable}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{graphicx}
\graphicspath{{./}{../image/}}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{epstopdf}
\usepackage[inline]{enumitem}
\usepackage{bm}

% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{lineno}
\linenumbers*[1]
\usepackage{setspace}


%% patches to make lineno work better with amsmath
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
	\expandafter\let\csname old#1\expandafter\endcsname\csname 
	#1\endcsname
	\expandafter\let\csname oldend#1\expandafter\endcsname\csname 
	end#1\endcsname
	\renewenvironment{#1}%
	{\linenomath\csname old#1\endcsname}%
	{\csname oldend#1\endcsname\endlinenomath}}%
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
	\patchAmsMathEnvironmentForLineno{#1}%
	\patchAmsMathEnvironmentForLineno{#1*}}%
\AtBeginDocument{%
	\patchBothAmsMathEnvironmentsForLineno{equation}%
	\patchBothAmsMathEnvironmentsForLineno{align}%
	\patchBothAmsMathEnvironmentsForLineno{flalign}%
	\patchBothAmsMathEnvironmentsForLineno{alignat}%
	\patchBothAmsMathEnvironmentsForLineno{gather}%
	\patchBothAmsMathEnvironmentsForLineno{multline}%
}

\allowdisplaybreaks

\title{Uncertainty Quantification in Deep Learning via Multiplier Bootstrap Methods}
\author[1]{Jun Yan}

\affil[1]{Department of Statistics, University of Connecticut, 
Storrs, CT 06269, USA}

\begin{document}
	
\maketitle
	
\begin{abstract}
Uncertainty quantification is a critical component of deploying deep
learning models in scientific and high-stakes applications. While deep
neural networks can achieve remarkable predictive performance, their
outputs often lack reliable measures of uncertainty, limiting their
practical utility in decision making. Existing approaches, including
Bayesian neural networks, ensemble methods, and calibration
techniques, offer partial solutions but face challenges in balancing
computational scalability with statistical validity. From a
statistical standpoint, bootstrap and resampling methods provide
well-established tools for quantifying uncertainty, yet their direct
application to modern deep networks has been limited. This work
investigates the use of multiplier bootstrap methods for uncertainty
quantification in deep learning. By leveraging scalable approximations
of estimating equations, we aim to develop practical procedures that
deliver valid uncertainty assessments for predictions from complex
models with millions of parameters. The proposed perspective seeks to
bridge the gap between statistical theory and the pressing need for
trustworthy uncertainty quantification in modern deep learning.


\bigskip
\noindent{\bf Keywords.}

\end{abstract}

\doublespacing

\section{Introduction}
\label{sec:intro}

Uncertainty quantification is fundamental to the scientific use of
machine learning and artificial intelligence. In high-stakes domains
such as medical imaging, autonomous driving, and climate prediction,
the reliability of model predictions is as important as their
accuracy. Deep neural networks have achieved impressive predictive
performance, yet their outputs often lack calibrated uncertainty
measures. Without trustworthy quantification of prediction uncertainty,
end users cannot meaningfully assess risk or make informed
decisions. This limitation has raised concerns from both scientific
and regulatory perspectives, highlighting the need for statistical
frameworks that integrate uncertainty into deep learning methods
\citep{kendall2017uncertainties, abdar2021review}.


A growing literature has investigated uncertainty estimation in deep
learning, spanning Bayesian neural networks, ensemble methods, and
post-hoc calibration techniques. Bayesian approaches offer
theoretically appealing formulations but are computationally
demanding, especially for large-scale networks
\citep{blundell2015weight, gal2016dropout}. Ensemble-based strategies
improve robustness but require substantial computational resources
\citep{lakshminarayanan2017simple}. Calibration methods such as
temperature scaling are more efficient but limited in scope
\citep{guo2017calibration}. Within medical imaging, most works focus
on predictive accuracy while offering heuristic measures of
uncertainty, often without rigorous statistical justification
\citep{mehrtash2020confidence, huang2021confidence}. From a broader
statistical perspective, methods for quantifying uncertainty in
complex models have long been studied, including the bootstrap,
multiplier resampling, and sandwich variance estimation
\citep{efron1994introduction, lahiri2003resampling, zhou2012multiplier}.


Despite these developments, there remains a gap between statistical
theory and practical uncertainty quantification in deep learning. Most
existing methods emphasize either computational feasibility or
theoretical rigor, but rarely both. In particular, scalable statistical
approaches that can provide valid uncertainty estimates for complex
models with millions of parameters are still lacking. Furthermore,
current evaluations largely emphasize prediction accuracy rather than
the reliability of uncertainty measures. This paper aims to bridge
these perspectives by exploring multiplier bootstrap methods in the
context of deep learning, with the goal of developing practical,
statistically principled approaches for uncertainty quantification.

\section{Related Work}

Uncertainty quantification in deep learning has been approached from
several methodological directions. The most prominent stream builds on
Bayesian formulations of neural networks, where weights are treated as
random variables and posterior distributions are approximated. Methods
such as variational inference and Monte Carlo dropout have been widely
used to make Bayesian inference scalable to modern architectures
\citep{blundell2015weight, gal2016dropout}. While these techniques
provide a principled framework for incorporating prior information and
posterior uncertainty, they remain computationally intensive, often
requiring significant approximations. Consequently, their practical
use has been largely confined to smaller models or settings where
computational budgets are less restrictive.


Another major line of work relies on ensembles of deep networks to
quantify predictive uncertainty. By training multiple models with
different initializations or data subsets, ensembles capture
variability in predictions that can be interpreted as uncertainty
\citep{lakshminarayanan2017simple}. Large-scale benchmarks under
dataset shift consistently find ensembles competitive or superior to
many approximate Bayesian methods and post-hoc calibration
\citep{ovadia2019can}. Explanatory analyses attribute part of this
advantage to functional diversity arising from distinct loss-landscape
modes explored by independent initializations \citep{fort2019deep},
while careful studies also document diminishing returns and metric
pitfalls when comparing ensemble variants \citep{ashukha2020pitfalls}.
Despite their strong empirical performance, ensembles incur training
and memory costs that scale with the number of members and, as
typically implemented, provide limited formal guarantees for interval
coverage or variance estimation. These trade-offs motivate
complementary approaches that retain statistical validation while
remaining computationally practical at scale.


Calibration methods form a third category, adjusting predictive
probabilities to better align with observed frequencies. Temperature
scaling offers a simple, effective post-hoc remedy for modern networks
\citep{guo2017calibration}. Beyond this, principled binary and
multiclass calibrators, including beta and Dirichlet calibration,
improve probability estimates across diverse models
\citep{kull2017beta, kull2019beyond}. For regression, calibrated
intervals can be obtained by post-processing any model, including deep
nets \citep{kuleshov2018accurate}. Training-time regularizers also
affect calibration: label smoothing and mixup generally reduce
overconfidence and improve expected calibration error
\citep{muller2019when, thulasidasan2019mixup}. However, post-hoc
calibration tuned on i.i.d.\ validation data may deteriorate under
distribution shift, where methods that account for epistemic
uncertainty (e.g., ensembles) tend to be more robust
\citep{ovadia2019can}. These results underscore both the usefulness
and the limitations of calibration as a comprehensive uncertainty
quantification strategy.


Conformal inference offers distribution-free predictive guarantees
under minimal assumptions, furnishing set- or interval-valued outputs
with finite-sample marginal coverage. Rooted in exchangeability and
nonconformity scoring, conformal methods are algorithm-agnostic and
scale naturally via data splitting \citep{vovk2005algorithmic,
lei2018distribution, angelopoulos2023conformal}. For regression, split
conformal intervals provide valid coverage broadly, while variants
improve efficiency and adaptivity: conformalized quantile regression
leverages conditional quantiles for heteroscedastic data
\citep{romano2019conformalized}, and the jackknife+ uses leave-one-out
predictions to sharpen coverage without fragile asymptotics
\citep{barber2021predictive}. For classification, set-valued
predictors control miscoverage while minimizing ambiguity, connecting
to calibrated probability outputs. These properties make conformal
methods attractive complements to probabilistic machine learning.


Applications in deep learning have further demonstrated the
feasibility of conformal approaches for modern high-dimensional
models. Early work combined conformal predictors with neural networks
to deliver valid uncertainty sets \citep{papadopoulos2007conformal}.
Recent developments extend the framework to image classification,
where predictive sets improve both calibration and interpretability
\citep{angelopoulos2021uncertainty}, and to robustness under
distributional shift \citep{stutz2022conformal}. Extensions with
PAC-style guarantees enhance reliability in complex architectures
\citep{park2022pac}. These advances show that conformal inference can
be efficiently integrated into deep models while retaining rigorous
coverage guarantees.


From the statistical perspective, resampling-based methods have long
served as a foundation for uncertainty assessment. The bootstrap
\citep{efron1994introduction} and its extensions, including block
bootstrap and resampling methods for dependent data
\citep{lahiri2003resampling}, offer flexible and widely applicable
tools. More recently, multiplier bootstrap techniques have been
developed to provide computationally efficient alternatives that avoid
explicit data resampling while retaining asymptotic validity
\citep{zhou2012multiplier, chen2020jackknife}. These approaches have
been successfully applied in high-dimensional regression, time series,
and survival analysis, highlighting their adaptability and
computational advantages. The question that naturally arises is
whether such statistically principled methods can be effectively
translated into the context of deep learning, where models often
involve millions of parameters and nonconvex optimization landscapes.


The preceding classes of methods highlight the breadth of approaches
to uncertainty quantification in deep learning. Yet none fully resolve
the central challenge of providing scalable, statistically justified
uncertainty measures for highly overparameterized models. Bayesian and
ensemble methods remain computationally demanding, calibration methods
lack variance guarantees, and conformal inference, while powerful, is
limited by exchangeability assumptions. Bootstrap and multiplier
bootstrap methods, by contrast, offer theoretically grounded and
computationally adaptable frameworks that have seen success in other
high-dimensional problems. This motivates our investigation into how
multiplier bootstrap techniques can be translated into the deep
learning setting. In the next section, we outline how the structure of
estimating equations in neural networks naturally connects to
multiplier resampling, and how this connection can be exploited to
develop practical procedures for uncertainty quantification.


\section{Multiplier Bootstrap with Deep Learning}

The multiplier bootstrap provides an efficient approximation to the
distribution of functionals defined through estimating equations. Let
$\hat{\theta}$ solve
\begin{equation}
g_n(\theta)=\frac{1}{n}\sum_{i=1}^n g(x_i,y_i;\theta)=0,
\label{eq:estimating}
\end{equation}
where $g(x_i,y_i;\theta)$ is the score contribution of observation $i$.
For a differentiable functional $T(\theta)$, the asymptotic variance
has the sandwich form
\begin{equation}
\operatorname{Var}\{T(\hat{\theta})\}\approx
G(\hat{\theta})\,H(\hat{\theta})^{-1}\,
\Sigma(\hat{\theta})\,H(\hat{\theta})^{-T}\,G(\hat{\theta})^\top,
\label{eq:sandwich}
\end{equation}
where $G(\hat{\theta})=\nabla_\theta T(\hat{\theta})$ and
$H(\hat{\theta})=\nabla_\theta g_n(\hat{\theta})$. Here
$\Sigma(\hat{\theta})=\operatorname{Var}\{g(x_i,y_i;\hat{\theta})\}$,
with the empirical counterpart
\begin{equation}
\widehat{\Sigma}(\hat{\theta})=
\frac{1}{n}\sum_{i=1}^n g(x_i,y_i;\hat{\theta})
\,g(x_i,y_i;\hat{\theta})^\top.
\label{eq:Sigmahat}
\end{equation}
Instead of re-estimating $\theta$ on resampled data, the multiplier
bootstrap perturbs the score with i.i.d.\ multipliers
$\{\xi_i\}_{i=1}^n$ satisfying $\mathbb{E}(\xi_i)=0$ and
$\operatorname{Var}(\xi_i)=1$,
\begin{equation}
g_n^*=\frac{1}{n}\sum_{i=1}^n \xi_i\,g(x_i,y_i;\hat{\theta}),
\label{eq:multiplier}
\end{equation}
and uses the linear expansion
\begin{equation}
\Delta T^*\approx G(\hat{\theta})\,H(\hat{\theta})^{-1}\,g_n^*,
\label{eq:linear}
\end{equation}
whose distribution consistently mimics that of
$T(\hat{\theta})-\mathbb{E}\{T(\hat{\theta})\}$ and reproduces the
sandwich variance in \eqref{eq:sandwich}.


In deep learning, the same structure arises from empirical risk
minimization. Given data $\{(x_i,y_i)\}_{i=1}^n$ and a network
$f(x;\theta)$, training minimizes
\begin{equation}
L_n(\theta)=\frac{1}{n}\sum_{i=1}^n
\ell\{y_i,f(x_i;\theta)\},
\label{eq:loss}
\end{equation}
so that \eqref{eq:estimating} holds with
$g(x_i,y_i;\theta)=\nabla_\theta \ell\{y_i,f(x_i;\theta)\}$. Prediction
at a new input $x_0$ is the functional
\begin{equation}
T(\theta;x_0)=f(x_0;\theta).
\label{eq:functional}
\end{equation}
For replicate index $b=1,\dots,B$, multipliers
$\{\xi_i^{(b)}\}$ produce $g_n^*(b)$ via \eqref{eq:multiplier}, and the
prediction replicate is
\begin{equation}
T\bigl(\hat{\theta}^*(b);x_0\bigr)
= T(\hat{\theta};x_0)+\Delta T^*(b),
\label{eq:replicate}
\end{equation}
with $\Delta T^*(b)$ given by \eqref{eq:linear}. The dispersion of
$\{T(\hat{\theta}^*(b);x_0)\}_{b=1}^B$ yields a bootstrap estimate of
the sandwich variance in \eqref{eq:sandwich} specialized to
\eqref{eq:functional}.


For regression, we combine the epistemic component estimated from the
replicates with an aleatoric term at $x_0$. Let
$\widehat{v}_{\mathrm{epi}}(x_0)$ be the sample variance of
$\{T(\hat{\theta}^*(b);x_0)\}_{b=1}^B$ and
$\hat{\sigma}^2(x_0)$ an estimate of the conditional noise variance.
A normal-approximation interval is
\begin{equation}
I_\alpha(x_0)=
\hat{T}(x_0)\pm z_{\alpha/2}\,
\bigl\{\widehat{v}_{\mathrm{epi}}(x_0)
+\hat{\sigma}^2(x_0)\bigr\}^{1/2},
\label{eq:reg_interval}
\end{equation}
while an empirical alternative uses the $\alpha/2$ and
$1-\alpha/2$ quantiles of
$\{T(\hat{\theta}^*(b);x_0)+\epsilon_b(x_0)\}$ with
$\epsilon_b(x_0)\sim \mathcal{N}\{0,\hat{\sigma}^2(x_0)\}$. For
classification, the replicates induce predictive probability vectors,
from which confidence scores or set-valued predictions are obtained by
thresholding their empirical distribution.

\begin{algorithm}[t]
\caption{Multiplier Bootstrap for Deep Learning Predictions}
\begin{algorithmic}[1]
\STATE Train the network to obtain $\hat{\theta}$ by minimizing
$L_n(\theta)$ in \eqref{eq:loss}.
\FOR{$b=1,\dots,B$}
  \STATE Generate multipliers $\{\xi_i^{(b)}\}_{i=1}^n$.
  \STATE Compute $g_n^*(b)$ as in \eqref{eq:multiplier}.
  \STATE Evaluate $\Delta T^*(b)$ via \eqref{eq:linear}.
  \STATE Form $T\bigl(\hat{\theta}^*(b);x_0\bigr)$ using
  \eqref{eq:replicate}.
\ENDFOR
\STATE Aggregate replicates to estimate variance, intervals, or
predictive sets as in \eqref{eq:sandwich} and \eqref{eq:reg_interval}.
\end{algorithmic}
\end{algorithm}

This formulation embeds multiplier bootstrap inference within deep
networks and produces prediction-level replicates that combine epistemic
and aleatoric effects in a principled way. The main computational
difficulty is evaluating $H(\hat{\theta})^{-1}g_n^*$ at scale; the next
section develops Hessian–vector product–based iterative solvers that
avoid forming or inverting $H(\hat{\theta})$ explicitly.


\section{Scalability and Computation}

The computational challenge comes from the evaluation of
$H(\hat{\theta})^{-1}g_n^*$ when the dimension of $\theta$ is high
like in deep learning settings.
Direct inversion of $H(\hat{\theta})$ is impossible since $p$ reaches
millions in modern networks. A scalable alternative is to use
Hessian–vector products (HVPs). For any vector $v \in \mathbb{R}^p$,
\[
H(\hat{\theta})v = \nabla_\theta^2 L_n(\hat{\theta})\,v,
\]
which can be computed by automatic differentiation at a cost comparable
to a single gradient evaluation. This enables iterative solvers that
require only matrix–vector products, without ever forming or inverting
$H(\hat{\theta})$ explicitly.


Two strategies are available. In the \emph{adjoint approach}, one solves
\[
H(\hat{\theta})u = G(\hat{\theta})^\top
\]
for $u$ using conjugate gradient with HVPs. Then each bootstrap replicate
is evaluated by
\[
\Delta T^*(b) \approx u^\top g_n^*(b),
\]
so the cost of solving is amortized once across all replicates. In the
\emph{forward approach}, one solves
\[
H(\hat{\theta})v^*(b) = g_n^*(b),
\]
and sets $\Delta T^*(b)=G(\hat{\theta})^\top v^*(b)$. This requires a
new conjugate gradient solve per replicate but avoids explicit
computation of $G(\hat{\theta})$ when it is high dimensional.


All required components are already available in deep learning
frameworks. Gradients of predictions with respect to parameters provide
$G(\hat{\theta})$, while HVPs are standard operations in autodiff. The
perturbed scores $g_n^*(b)$ can be generated at the mini-batch level by
assigning multipliers to observations within each batch, ensuring
scalability to large datasets. The overall cost depends on the number of
conjugate gradient iterations $K$: the adjoint approach requires
$O(K\cdot C_{\text{HVP}})$ once plus cheap inner products for each
replicate, while the forward approach requires $O(K\cdot C_{\text{HVP}})$
per replicate. Both are dramatically more efficient than retraining
ensembles or sampling Bayesian posteriors, yet retain the formal
connection to the sandwich variance through multiplier resampling.

\section{Simulation Studies}

We evaluate the proposed approach through simulation experiments
constructed to reflect the overparameterization of modern deep
networks while retaining access to ground truth predictive
distributions. Data are generated from a fixed teacher network
$f^\star(x)$, taken to be a two–layer ReLU model with width $m$ and
randomly drawn weights. Inputs are sampled from
$x\sim\mathcal{N}(0,I_d)$ and responses follow
$y\mid x \sim \mathcal{N}(f^\star(x),\sigma^2(x))$, where the variance
is either homoskedastic, $\sigma^2(x)=\sigma_0^2$, or heteroskedastic,
$\sigma^2(x)=\sigma_0^2\{1+\alpha \|x\|/\sqrt{d}\}^2$. The student
network is a multilayer perceptron with width $M\gg m$ and depth $L$,
so that the number of parameters $p$ reaches into the millions.
Training is carried out by stochastic gradient descent or Adam with
early stopping, closely matching practical conditions. By design, the
conditional distribution of $y\mid x$ is known, enabling direct
evaluation of interval coverage in settings with high $p$ and moderate
sample size $n$.


After fitting the student model, prediction-level multiplier bootstrap
replicates are generated using the adjoint or forward computational
strategies from Section~4. For each test input $x$, we compute
bootstrap predictions $\{f(x;\hat\theta^{*(b)})\}_{b=1}^B$ and form
either normal-approximation intervals, based on the estimated variance
\[
  \widehat v_{\mathrm{pred}}(x) =
  \widehat v_{\mathrm{epi}}(x) + \sigma^2(x),
\]
or empirical quantile intervals obtained by adding noise
$\epsilon_b(x)\sim\mathcal{N}(0,\sigma^2(x))$ to the replicates. The
performance of these intervals is assessed on an independent test set
by computing empirical coverage at nominal levels, average interval
length as a measure of sharpness, and calibration curves based on
predicted probabilities. Experimental factors varied include training
sample size $n$, input dimension $d$, noise level $\sigma_0$,
heteroskedasticity parameter $\alpha$, network width $M$ and depth $L$,
and teacher–student mismatch in activation functions. Comparisons are
made against baselines such as Monte Carlo dropout and deep ensembles,
providing context for both statistical validity and computational
efficiency.


To summarize results, we report marginal and conditional coverage at
nominal $90\%$ and $95\%$ levels, average interval length, and wall-
clock compute, alongside calibration diagnostics. Conditional coverage
is stratified by $\|x\|$, by $f^\star(x)$, and by noise level bins to
reveal heterogeneity under heteroskedasticity. We include ablations on
solver tolerance and iteration cap $(K)$, mini-batch size for forming
$g^{*}$, number of replicates $(B)$, and the choice of multiplier law,
to assess robustness–efficiency trade-offs. Baselines (MC dropout,
deep ensembles, and a last-layer Laplace approximation) are run under
matched training budgets. We expect multiplier-bootstrap intervals to
track nominal coverage closely across regimes, with mild under- or
over-coverage when teacher–student mismatch is severe or $n$ is small,
and to produce shorter intervals than baselines at comparable compute.
We also anticipate stable conditional coverage across $\|x\|$ bins
once heteroskedastic variance is incorporated, demonstrating that the
method delivers calibrated, sharp prediction intervals in the
overparameterized setting.

\section{Application}

\section{Discussion}



\bibliographystyle{chicago}
\bibliography{refs}
	
\end{document}

%%% LocalWords: assortativity
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% ispell-personal-dictionary: ".aspell.en.pws"
%%% fill-column: 68
%%% eval: (auto-fill-mode 1)
%%% End:
